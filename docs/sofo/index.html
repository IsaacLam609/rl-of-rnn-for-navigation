<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml"><head><title>index (sofo.index)</title><meta charset="utf-8"/><link rel="stylesheet" href="../odoc.support/odoc.css"/><meta name="generator" content="odoc 3.0.0"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><script src="../odoc.support/highlight.pack.js"></script><script>hljs.initHighlightingOnLoad();</script><script>let base_url = '../';
let search_urls = ['db.js','../sherlodoc.js'];
</script><script src="../odoc.support/odoc_search.js" defer="defer"></script></head><body class="odoc"><nav class="odoc-nav"><a href="../index.html">Up</a> â€“ <a href="../index.html">Index</a> &#x00BB; sofo</nav><div class="odoc-search"><div class="search-inner"><input class="search-bar" placeholder="ðŸ”Ž Type '/' to search..."/><div class="search-snake"></div><div class="search-result"></div></div></div><header class="odoc-preamble"><h1 id="sofo:-second-order-forward-mode-optimization"><a href="#sofo:-second-order-forward-mode-optimization" class="anchor"></a>SOFO: second-order forward-mode optimization</h1><p>Library description: This is the library for the SOFO optimizer, which is a second-order subspace method using batched forward-mode differentitation.</p><ul class="modules"><li><a href="Sofo/Optimizer/index.html"><code>Sofo.Optimizer</code></a> <span class="synopsis">Optimizers used for training the networks.</span></li><li><a href="Sofo/Loss/index.html"><code>Sofo.Loss</code></a> <span class="synopsis">Loss function to be minimised.</span></li><li><a href="Sofo/Wrapper/index.html"><code>Sofo.Wrapper</code></a> <span class="synopsis">Wrappers define the forward computational graph.</span></li><li><a href="Sofo/Convenience/index.html"><code>Sofo.Convenience</code></a> <span class="synopsis">Contains convenience functions.</span></li></ul><p>Here is a small MLP example. First, we define the parameter module.</p><pre class="language-ocaml"><code>open Base
open Forward_torch
open Sofo

include struct
   type 'a p = {
      weights : 'a
    ; biases: 'a
    }
end [@@deriving prms]
</code></pre><p>We then use <code>Make</code> to create a parameter module for a single MLP layer:</p><pre class="language-ocaml"><code>module MLP_Layer = Prms.Make (Prms.P)</code></pre><p>and even make a list of those for a multi-layer MLP:</p><pre class="language-ocaml"><code>module MLP = Prms.List (MLP_Layer)</code></pre><p>Then, we can instantiate the computational graph (all layers of the same size for simpliticy):</p><pre class="language-ocaml"><code>let n_layers = 3
let size = 100

module Three_Layer_MLP = struct
  module P = MLP

  type input = Maths.t

  let f ~(prms:MLP.M.t) x = List.fold prms ~init:x ~f:(fun accu { weights; biases } -&gt;
      Maths.(relu (accu *@ weights + biases)))

  let init : MLP.tagged = List.init n_layers ~f:(fun _ -&gt;
    let weights = Tensor.(randn [size; size] * f Float.(1. / sqrt (of_int size))) in
    let biases = Tensor.zeros [1; size] in
    { weights = Prms.free weights; biases = Prms.const weights })
end</code></pre><p>where <code>f</code> defines the forward computation and <code>init</code> defines how we initialise the parameters.</p><p>Now we need to define our wrapper, which is a feedforward net (<a href="Sofo/Wrapper/Feedforward/index.html"><code>Sofo.Wrapper.Feedforward</code></a>) whose computational graph is given by <code>Three_Layer_MLP</code> and we use the mean-squared error (with scaling=1) as the loss function (<a href="Sofo/Loss/MSE/index.html"><code>Sofo.Loss.MSE</code></a>). It is also possible (and sometimes necessary) to define your own wrapper and loss functions, with the types <a href="Sofo/Wrapper/module-type-T/index.html"><code>Sofo.Wrapper.T</code></a> and <a href="Sofo/Loss/module-type-T/index.html"><code>Sofo.Loss.T</code></a> respectively.</p><pre class="language-ocaml"><code>module FF =
  Wrapper.Feedforward
    (Three_Layer_MLP)
    (Loss.MSE (struct
         let scaling_factor = 1.
       end))</code></pre><p>Next, we define SOFO as our optimizer</p><pre class="language-ocaml"><code>module O = Optimizer.SOFO (FF)</code></pre><p>and configure it with</p><pre class="language-ocaml"><code>let config =
  Optimizer.Config.SOFO.
    { base = Optimizer.Config.Base.default
    ; learning_rate = Some 1.
    ; n_tangents = 10
    ; rank_one = false
    ; damping = None
    ; momentum = None
    }</code></pre><p>where we have chosen a learning rate of 1 and the number of tangents to be 10. Finally, we can run the optimization loop</p><pre class="language-ocaml"><code>let max_iter = 10000
let rec loop ~t ~state =
  (* define your own data here, which needs to be of type Three_Layer_MLP.input * Three_Layer_MLP.input *)
  let data = generate_data batch_size in
  let loss, new_state = O.step ~config ~state ~data ~args:() in
  if t % 100 = 0
  then (
    Convenience.print [%message (t : int) (loss : float)];
  if t &lt; max_iter then loop ~t:(t + 1)  ~state:new_state

let _ =  loop ~t:0 ~state:(O.init ~config (Three_Layer_MLP.init ))</code></pre></header><div class="odoc-content"></div></body></html>
