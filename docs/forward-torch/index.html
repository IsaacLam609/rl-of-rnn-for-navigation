<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml"><head><title>index (forward-torch.index)</title><meta charset="utf-8"/><link rel="stylesheet" href="../odoc.support/odoc.css"/><meta name="generator" content="odoc 3.0.0"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><script src="../odoc.support/highlight.pack.js"></script><script>hljs.initHighlightingOnLoad();</script><script>let base_url = '../';
let search_urls = ['db.js','../sherlodoc.js'];
</script><script src="../odoc.support/odoc_search.js" defer="defer"></script></head><body class="odoc"><nav class="odoc-nav"><a href="../index.html">Up</a> ‚Äì <a href="../index.html">Index</a> &#x00BB; forward-torch</nav><div class="odoc-search"><div class="search-inner"><input class="search-bar" placeholder="üîé Type '/' to search..."/><div class="search-snake"></div><div class="search-result"></div></div></div><header class="odoc-preamble"><h1 id="batched-forward-mode-differentiation"><a href="#batched-forward-mode-differentiation" class="anchor"></a>Batched forward-mode differentiation</h1><p>This libraries enables GPU-accelerated batched forward-mode automatic differentiation (SOFO's main workhorse).</p><ul class="modules"><li><a href="Forward_torch/Maths/index.html"><code>Forward_torch.Maths</code></a> <span class="synopsis">Contains all the maths operators that automate batched forward-mode differentation.</span></li><li><a href="Forward_torch/Prms/index.html"><code>Forward_torch.Prms</code></a> <span class="synopsis">Deals with arbitrary (nested) parameter records.</span></li></ul><p>Here is a small example to illustrate obtain a gradient sketch for a multi-layer MLP. Start by defining your parameter module using the <code>ppx</code> to automate the generation of all functions enabling convenient handling of arbitrary parameter structures (don't forget to add the <code>(preprocess (pps ppx-forward-torch))</code> stanza in your <code>dune</code> file):</p><pre class="language-ocaml"><code>open Base
open Forward_torch

include struct
   type 'a p = {
      weights : 'a
    ; biases: 'a
    }
end [@@deriving prms]</code></pre><p>The <code>ppx</code> automatically generates a functor (<code>Make</code>) which accepts a module of type <a href="Forward_torch/Prms/module-type-T/index.html"><code>Forward_torch.Prms.T</code></a> for any independent leaf type (here, we only have <code>'a</code>). The most primitive of all <a href="Forward_torch/Prms/module-type-T/index.html"><code>Forward_torch.Prms.T</code></a> modules is the singleton <a href="Forward_torch/Prms/P/index.html"><code>Forward_torch.Prms.P</code></a>, describing a single parameter tensor. We can use <code>Make</code> as follows to create a parameter module for a single MLP layer:</p><pre class="language-ocaml"><code>module MLP_Layer = Prms.Make (Prms.P)</code></pre><p>and even make a whole list of those for a multi-layer MLP:</p><pre class="language-ocaml"><code>module MLP = Prms.List (MLP_Layer)</code></pre><p>This module <code>MLP</code> contains loads of convenience functions to manipulate lists of weights and biases, in a type-safe way. It exposes two main types:</p><ul><li><code>MLP.M.t = Maths.t P.p</code> where the <code>'a</code> leaves of the parameter tree are <a href="Forward_torch/Maths/index.html#type-t"><code>Forward_torch.Maths.t</code></a> types, i.e. they may carry tangents (directional derivatives)</li><li><code>MLP.T.t = Tensor.t P.p</code> where the <code>'a</code> leaves of the parameter tree are simple <code>Torch.Tensor.t</code></li></ul><p>Then, we can express the network's forward pass using <code>Maths</code> operations:</p><pre class="language-ocaml"><code>let forward ~(prms:MLP.M.t) x =
  List.fold prms ~init:x ~f:(fun accu { weights; biases } -&gt;
      Maths.(relu (accu *@ weights + biases)))</code></pre><p>Now let's initialise our parameters (all layers of the same size for simplicity).</p><pre class="language-ocaml"><code>let n_layers = 3
let size = 10

let prms : MLP.tagged = List.init n_layers ~f:(fun _ -&gt;
  let weights = Tensor.(randn [size; size] * f Float.(1. / sqrt (of_int size))) in
  let biases = Tensor.zeros [1; size] in
  { weights = Prms.free weights; biases = Prms.const weights })</code></pre><p>Note that parameters are ‚Äútagged‚Äù as being either free parameters, or constant parameters (in which case they will not carry directional derivatives, and SOFO won't update them).</p><p>Let us now attached a bunch of 16 random tangents to those parameters, and automatically compute the corresponding directional derviatives of the output of the network:</p><pre class="language-ocaml"><code>let n_tangents = 128
let batch_size = 256

let prms : MLP.M.t =
  let v = MLP.gaussian_like_k ~k:n_tangents prms in
  MLP.make_dual prms ~v

(* random inputs to our MLP *)
let x = Tensor.randn [batch_size; size] |&gt; Maths.const

let y : Maths.t = forward ~prms x

(* the tangents of [y] contain all directional derivatives, from which we can e.g. extract a sketch 
   of the Gauss-Newton matrix etc (see {!Sofo}) *)
let yt : Tensor.t = Maths.tangent_exn y</code></pre></header><div class="odoc-content"></div></body></html>
